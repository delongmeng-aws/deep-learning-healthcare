{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Failure Prediction using Neural Network \n",
    "\n",
    "## Overview\n",
    "\n",
    "In this project, we will use [PyTorch](https://pytorch.org), a framework for building and training neural networks, to train a simple neural network on **Heart Failure Prediction**. PyTorch in a lot of ways behaves like the arrays from Numpy. These Numpy arrays, after all, are just tensors. PyTorch takes these tensors and makes it simple to move them to GPUs for the faster processing needed when training neural networks. It also provides a module that automatically calculates gradients (for backpropagation) and another module specifically for building neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "DATA_PATH = \"lib/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 PyTorch Basics\n",
    "\n",
    "It turns out neural network computations are just a bunch of linear algebra operations on tensors, a generalization of matrices. A vector is a 1-dimensional tensor, a matrix is a 2-dimensional tensor, an array with three indices is a 3-dimensional tensor (RGB color images for example). The fundamental data structure for neural networks are tensors and PyTorch (as well as pretty much every other deep learning framework) is built around tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 ReLU Implementation from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def relu(x):\n",
    "\n",
    "    \"\"\" \n",
    "    Implement a ReLU activation function from scratch.\n",
    "    REFERENCE: https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU\n",
    "    input\n",
    "        x: torch.Tensor\n",
    "    output\n",
    "        relu(x): torch.Tensor\n",
    "    \"\"\"\n",
    "    zero_tensor = torch.zeros_like(x)\n",
    "    return torch.maximum(zero_tensor,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Sigmoid Implementation from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "\n",
    "    \"\"\" \n",
    "    Implement a Sigmoid activation function from scratch.\n",
    "    input\n",
    "        x: torch.Tensor\n",
    "    output\n",
    "        sigmoid(x): torch.Tensor\n",
    "    \"\"\"\n",
    "    tensor_ones = torch.ones_like(x)\n",
    "    return torch.div(tensor_ones, tensor_ones + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Softmax Implementation from scratch\n",
    "\n",
    "Note that softmax degenerates to sigmoid when we have 2 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "\n",
    "    \"\"\" \n",
    "    Implement a Softmax activation function from scratch.\n",
    "    input\n",
    "        x: torch.Tensor, 2D matrix\n",
    "    output\n",
    "        softmax(x): torch.Tensor, 2D matrix with sum over rows is 1\n",
    "    \"\"\"\n",
    "    return torch.exp(x) / torch.sum(torch.exp(x), dim = 1, keepdim = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Single layer network with sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us try to use the `sigmoid` function to calculate the output for a simple single layer network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some data\n",
    "# Features are 5 random normal variables\n",
    "features = torch.randn((1, 5))\n",
    "# weights for our data, random normal variables again\n",
    "weights = torch.randn_like(features)\n",
    "# and a bias term\n",
    "bias = torch.randn((1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above I generated data we can use to get the output of our simple network. This is all just random for now, going forward we will start using normal data.\n",
    "\n",
    "`features = torch.randn((1, 5))` creates a tensor with shape (1, 5), one row and five columns, that contains values randomly distributed according to the normal distribution with a mean of zero and standard deviation of one.\n",
    "\n",
    "`weights = torch.randn_like(features)` creates another tensor with the same shape as features, again containing values from a normal distribution.\n",
    "\n",
    "`bias = torch.randn((1, 1))` creates a single value from a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can use the generated data to calculate the output of this simple single layer network. Input features are `features`, weights are `weights`, and bias are `bias`. Use `sigmoid` as the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def single_layer_network(features, weights, bias):\n",
    "\n",
    "    \"\"\" \n",
    "    Calculate the output of this simple single layer network.\n",
    "    input\n",
    "        features: torch.Tensor\n",
    "        weights: torch.Tensor\n",
    "        bias: torch.Tensor\n",
    "    output\n",
    "        output of a sinlge layer network: torch.Tensor\n",
    "    \"\"\"\n",
    "    return sigmoid(features.matmul(weights.T) + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is how we can calculate the output for a sinlge layer. The real power of this algorithm happens when we start stacking these individual units into layers and stacks of layers, into a network of neurons. The output of one layer of neurons becomes the input for the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 NN with PyTorch\n",
    "\n",
    "Deep learning networks tend to be massive with dozens or hundreds of layers, that is where the term \"deep\" comes from. PyTorch has a nice module `nn` that provides a nice way to efficiently build large neural networks.\n",
    "\n",
    "In this project, we will train a neural network to predict heart failure. The data has been processed and saved in SVMLight format under `DATA_PATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_svmlight.train  features_svmlight.val\r\n"
     ]
    }
   ],
   "source": [
    "!ls {DATA_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: torch.Size([2485, 1473])\n",
      "Y_train shape: torch.Size([2485])\n",
      "X_val shape: torch.Size([604, 1473])\n",
      "Y_val shape: torch.Size([604])\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "\"\"\" load SVMLight data \"\"\"\n",
    "# training data\n",
    "X_train, Y_train = utils.get_data_from_svmlight(DATA_PATH + \"features_svmlight.train\")\n",
    "# validation data\n",
    "X_val, Y_val = utils.get_data_from_svmlight(DATA_PATH + \"features_svmlight.val\")\n",
    "\n",
    "\"\"\" convert to torch.tensor \"\"\"\n",
    "X_train = torch.from_numpy(X_train.toarray()).type(torch.float)\n",
    "Y_train = torch.from_numpy(Y_train).type(torch.float)\n",
    "X_val = torch.from_numpy(X_val.toarray()).type(torch.float)\n",
    "Y_val = torch.from_numpy(Y_val).type(torch.float)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"Y_train shape:\", Y_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"Y_val shape:\", Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create a `TensorDataset` to wrap those tensors. (https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train_dataset: 2485\n",
      "Size of val_dataset: 604\n",
      "(tensor([0., 0., 0.,  ..., 0., 0., 0.]), tensor(0.))\n",
      "[torch.Size([1473]), torch.Size([])]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "val_dataset = TensorDataset(X_val, Y_val)\n",
    "\n",
    "print(\"Size of train_dataset:\", len(train_dataset))\n",
    "print(\"Size of val_dataset:\", len(val_dataset))\n",
    "\n",
    "#we can index train_dataset to get a (data, label) tuple\n",
    "print(train_dataset[0])\n",
    "print([_t.shape for _t in train_dataset[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will load the dataset into a dataloader so that we can we can use it to loop through the dataset for training and validating. (https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of train batches: 78\n",
      "# of val batchse: 19\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# how many samples per batch to load\n",
    "batch_size = 32\n",
    "\n",
    "# prepare dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "print(\"# of train batches:\", len(train_loader))\n",
    "print(\"# of val batchse:\", len(val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data loader is created with a batch size of $32$, and `shuffle=True`. \n",
    "\n",
    "The batch size is the number of images we get in one iteration from the data loader and pass through our network, often called a batch. \n",
    "\n",
    "And `shuffle=True` tells it to shuffle the dataset every time we start going through the data loader again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of a batch x: torch.Size([32, 1473])\n",
      "Shape of a batch y: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "train_iter = iter(train_loader)\n",
    "x, y = next(train_iter)\n",
    "\n",
    "print('Shape of a batch x:', x.shape)\n",
    "print('Shape of a batch y:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Build the Model\n",
    "\n",
    "Now, let us build a real NN model. For each patient, the NN model will take an input tensor of 1473-dim, and produce an output tensor of 1-dim (0 for normal, 1 for heart failure). The detailed model architecture is shown in the table below.\n",
    "\n",
    "Layers | Configuration | Activation Function | Output Dimension (batch, feature)\n",
    "--- | --- | --- | ---\n",
    "fully connected | input size 1473, output size 64 | ReLU | (32, 64)\n",
    "fully connected | input size 64, output size 32 | ReLU | (32, 32)\n",
    "dropout | probability 0.5 | - | (32, 32)\n",
    "fully connected | input size 32, output size 1 | Sigmoid | (32, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=1473, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc3): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Build the MLP shown above using `nn.Linear`, `nn.Dropout`, `torch.relu`, `torch.sigmoid`.\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(1473, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.fc3(self.dropout(self.fc2(torch.relu(self.fc1(x))))))\n",
    "\n",
    "# initialize the NN\n",
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a network, let's see what happens when we pass in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x shape: torch.Size([32, 1473])\n",
      "Output shape:  torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "# Grab some data \n",
    "train_iter = iter(train_loader)\n",
    "x, y = next(train_iter)\n",
    "\n",
    "# Forward pass through the network\n",
    "output = model.forward(x)\n",
    "\n",
    "print('Input x shape:', x.shape)\n",
    "print('Output shape: ', output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Train the Network\n",
    "\n",
    "In this step, we will train the NN model. \n",
    "\n",
    "Neural networks with non-linear activations work like universal function approximators. There is some function that maps the input to the output. The power of neural networks is that we can train them to approximate this function, and basically any function given enough data and compute time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses in PyTorch.\n",
    "\n",
    "Let us start by seeing how we calculate the loss with PyTorch. Through the `nn.module`, PyTorch provides losses such as the binary cross-entropy loss (`nn.BCELoss`). The loss is usually assigned to `criterion`. \n",
    "\n",
    "As noted in the last part, with a classification problem such as Heart Failure Prediction, we are using the Sigmoid function to predict heart failure probability. With a Sigmoid output, we want to use binary cross-entropy as the loss. To actually calculate the loss, we first define the criterion then pass in the output of the network and the correct labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss (BCELoss), assign it to `criterion`.\n",
    "# REFERENCE: https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html#torch.nn.BCELoss\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer in PyTorch.\n",
    "\n",
    "Optimizer can update the weights with the gradients. We can get these from PyTorch's `optim` package. For example we can use stochastic gradient descent with `optim.SGD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer (SGD) with learning rate 0.01, assign it to `optimizer`.\n",
    "# REFERENCE: https://pytorch.org/docs/stable/optim.html\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us train the NN model we previously created.\n",
    "\n",
    "First, let us implement the `evaluate` function that will be called to evaluate the model performance when training.\n",
    "\n",
    "***Note:*** For prediction, probability > 0.5 is considered class 1 otherwise class 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "\n",
    "#input: Y_score,Y_pred,Y_true\n",
    "#output: accuracy, auc, precision, recall, f1-score\n",
    "def classification_metrics(Y_score, Y_pred, Y_true):\n",
    "    acc, auc, precision, recall, f1score = accuracy_score(Y_true, Y_pred), \\\n",
    "                                           roc_auc_score(Y_true, Y_score), \\\n",
    "                                           precision_score(Y_true, Y_pred), \\\n",
    "                                           recall_score(Y_true, Y_pred), \\\n",
    "                                           f1_score(Y_true, Y_pred)\n",
    "    return acc, auc, precision, recall, f1score\n",
    "\n",
    "#input: model, loader\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_y_true = torch.LongTensor()\n",
    "    all_y_pred = torch.LongTensor()\n",
    "    all_y_score = torch.FloatTensor()\n",
    "    for x, y in loader:\n",
    "        y_hat = model(x)\n",
    "        # convert shape from [batch size, 1] to [batch size]\n",
    "        y_hat = y_hat.view(y_hat.shape[0])\n",
    "\n",
    "        # obtain the predicted class (0, 1) by comparing y_hat against 0.5, and assign the predicted class to y_pred.\n",
    "        y_pred = [1 if y >= 0.5 else 0 for y in y_hat]\n",
    "        y_pred = torch.from_numpy(np.asarray(y_pred))\n",
    "        \n",
    "        all_y_true = torch.cat((all_y_true, y.to('cpu').long()), dim=0)\n",
    "        all_y_pred = torch.cat((all_y_pred,  y_pred.to('cpu').long()), dim=0)\n",
    "        all_y_score = torch.cat((all_y_score,  y_hat.to('cpu')), dim=0)\n",
    "        \n",
    "    acc, auc, precision, recall, f1 = classification_metrics(all_y_score.detach().numpy(), \n",
    "                                                             all_y_pred.detach().numpy(), \n",
    "                                                             all_y_true.detach().numpy())\n",
    "    print(f\"acc: {acc:.3f}, auc: {auc:.3f}, precision: {precision:.3f}, recall: {recall:.3f}, f1: {f1:.3f}\")\n",
    "    return acc, auc, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model perfomance before training:\n",
      "acc: 0.417, auc: 0.552, precision: 0.000, recall: 0.000, f1: 0.000\n",
      "acc: 0.392, auc: 0.517, precision: 0.000, recall: 0.000, f1: 0.000\n"
     ]
    }
   ],
   "source": [
    "print(\"model perfomance before training:\")\n",
    "# initialized the model\n",
    "# model = Net()\n",
    "auc_train_init = evaluate(model, train_loader)[1]\n",
    "auc_val_init = evaluate(model, val_loader)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to train the model:\n",
    "- Clear the gradients of all optimized variables\n",
    "- Forward pass: compute predicted outputs by passing inputs to the model\n",
    "- Calculate the loss\n",
    "- Backward pass: compute gradient of the loss with respect to model parameters\n",
    "- Perform a single optimization step (parameter update)\n",
    "- Update average training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 0.701075\n",
      "acc: 0.445, auc: 0.556, precision: 0.808, recall: 0.114, f1: 0.200\n",
      "Epoch: 20 \tTraining Loss: 0.649222\n",
      "acc: 0.614, auc: 0.683, precision: 0.612, recall: 0.997, f1: 0.759\n",
      "Epoch: 40 \tTraining Loss: 0.547752\n",
      "acc: 0.712, auc: 0.725, precision: 0.704, recall: 0.907, f1: 0.793\n",
      "Epoch: 60 \tTraining Loss: 0.480068\n",
      "acc: 0.709, auc: 0.739, precision: 0.727, recall: 0.834, f1: 0.777\n",
      "Epoch: 80 \tTraining Loss: 0.414760\n",
      "acc: 0.714, auc: 0.738, precision: 0.727, recall: 0.847, f1: 0.782\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 100\n",
    "\n",
    "# prep model for training\n",
    "model.train()\n",
    "\n",
    "train_loss_arr = []\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    train_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        # Step 1. clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Step 2. perform forward pass using `model`, save the output to y_hat\n",
    "        y_hat = model(x)\n",
    "        y_hat = y_hat.view(y_hat.shape[0])\n",
    "        \n",
    "        # Step 3. calculate the loss using `criterion`, save the output to loss        \n",
    "        loss = criterion(y_hat, y)\n",
    "        \n",
    "        # Step 4. backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Step 5. optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Step 6. record loss\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        train_loss_arr.append(np.mean(train_loss))\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))\n",
    "        evaluate(model, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "398.390625px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "524px",
    "left": "1423px",
    "right": "20px",
    "top": "120px",
    "width": "348px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
