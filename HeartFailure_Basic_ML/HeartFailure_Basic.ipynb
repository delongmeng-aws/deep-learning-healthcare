{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Failure Prediction - Basic Models\n",
    "\n",
    "## Overview\n",
    "\n",
    "Preparing the data, computing basic statistics and constructing simple models are essential steps for data science practice. In this activity, we will use clinical data as raw input to perform **Heart Failure Prediction**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "DATA_PATH = \"lib/data/\"\n",
    "TRAIN_DATA_PATH = DATA_PATH + \"train/\"\n",
    "VAL_DATA_PATH = DATA_PATH + \"val/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Data\n",
    "\n",
    "For this project, we will be using a clinical dataset synthesized from [MIMIC-III](https://www.nature.com/articles/sdata201635).\n",
    "\n",
    "Navigate to `TRAIN_DATA_PATH`. There are three CSV files which will be the input data in this homework. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event_feature_map.csv  events.csv  hf_events.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls $TRAIN_DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**events.csv**\n",
    "\n",
    "The data provided in *events.csv* are event sequences. Each line of this file consists of a tuple with the format *(pid, event_id, vid, value)*. \n",
    "\n",
    "For example, \n",
    "\n",
    "```\n",
    "33,DIAG_244,0,1\n",
    "33,DIAG_414,0,1\n",
    "33,DIAG_427,0,1\n",
    "33,LAB_50971,0,1\n",
    "33,LAB_50931,0,1\n",
    "33,LAB_50812,1,1\n",
    "33,DIAG_425,1,1\n",
    "33,DIAG_427,1,1\n",
    "33,DRUG_0,1,1\n",
    "33,DRUG_3,1,1\n",
    "```\n",
    "\n",
    "- **pid**: De-identified patient identier. For example, the patient in the example above has pid 33. \n",
    "- **event_id**: Clinical event identifier. For example, DIAG_244 means the patient was diagnosed of disease with ICD9 code [244](http://www.icd9data.com/2013/Volume1/240-279/240-246/244/244.htm); LAB_50971 means that the laboratory test with code 50971 was conducted on the patient; and DRUG_0 means that a drug with code 0 was prescribed to the patient. Corresponding lab (drug) names can be found in `{DATA_PATH}/lab_list.txt` (`{DATA_PATH}/drug_list.txt`).\n",
    "- **vid**: Visit identifier. For example, the patient has two visits in total. Note that vid is ordinal. That is, visits with bigger vid occour after that with smaller vid.\n",
    "- **value**: Contains the value associated to an event (always 1 in the synthesized dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**hf_events.csv**\n",
    "\n",
    "The data provided in *hf_events.csv* contains pid of patients who have been diagnosed with heart failure (i.e., DIAG_398, DIAG_402, DIAG_404, DIAG_428) in at least one visit. They are in the form of a tuple with the format *(pid, vid, label)*. For example,\n",
    "\n",
    "```\n",
    "156,0,1\n",
    "181,1,1\n",
    "```\n",
    "\n",
    "The vid indicates the index of the first visit with heart failure of that patient and a label of 1 indicates the presence of heart failure. **Note that only patients with heart failure are included in this file. Patients who are not mentioned in this file have never been diagnosed with heart failure.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**event_feature_map.csv**\n",
    "\n",
    "The *event_feature_map.csv* is a map from an event_id to an integer index. This file contains *(idx, event_id)* pairs for all event ids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Descriptive Statistics\n",
    "\n",
    "Before starting analytic modeling, it is good to get descriptive statistics of the input raw data. We will write code that computes various metrics on the data described previously.\n",
    "\n",
    "The definition of terms used in the result table are described below:\n",
    "\n",
    "- **Event count**: Number of events recorded for a given patient.\n",
    "- **Encounter count**: Number of visits recorded for a given patient.\n",
    "\n",
    "Note that every line in the input file is an event, while each visit consists of multiple events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "def read_csv(filepath=TRAIN_DATA_PATH):\n",
    "\n",
    "    '''\n",
    "    Read the events.csv and hf_events.csv files. \n",
    "    Variables returned from this function are passed as input to the metric functions.\n",
    "    '''\n",
    "    \n",
    "    events = pd.read_csv(filepath + 'events.csv')\n",
    "    hf = pd.read_csv(filepath + 'hf_events.csv')\n",
    "\n",
    "    return events, hf\n",
    "\n",
    "def event_count_metrics(events, hf):\n",
    "\n",
    "    '''\n",
    "    Return the event count metrics.\n",
    "    Event count is defined as the number of events recorded for a given patient.\n",
    "    '''\n",
    "    event_counts_df = events.pid.value_counts().rename_axis('pid').reset_index(name='event_counts')\n",
    "    hf_event_counts, norm_event_counts = [], []\n",
    "    for index, row in event_counts_df.iterrows():\n",
    "        if row.pid in list(hf.pid):\n",
    "            hf_event_counts.append(row.event_counts)\n",
    "        else:\n",
    "            norm_event_counts.append(row.event_counts)\n",
    "\n",
    "    avg_hf_event_count = np.mean(hf_event_counts)\n",
    "    max_hf_event_count = np.max(hf_event_counts)\n",
    "    min_hf_event_count = np.min(hf_event_counts)\n",
    "    avg_norm_event_count = np.mean(norm_event_counts)\n",
    "    max_norm_event_count = np.max(norm_event_counts)\n",
    "    min_norm_event_count = np.min(norm_event_counts)    \n",
    "\n",
    "    return avg_hf_event_count, max_hf_event_count, min_hf_event_count, \\\n",
    "           avg_norm_event_count, max_norm_event_count, min_norm_event_count\n",
    "\n",
    "def encounter_count_metrics(events, hf):\n",
    "\n",
    "    '''\n",
    "    Return the encounter count metrics.\n",
    "    Encounter count is defined as the number of visits recorded for a given patient. \n",
    "    '''\n",
    "    encounter_counts_df = events.drop_duplicates(subset=['pid', 'vid']).pid.value_counts().rename_axis('pid').reset_index(name='encounter_counts')\n",
    "    hf_encounter_counts, norm_encounter_counts = [], []\n",
    "    for index, row in encounter_counts_df.iterrows():\n",
    "        if row.pid in list(hf.pid):\n",
    "            hf_encounter_counts.append(row.encounter_counts)\n",
    "        else:\n",
    "            norm_encounter_counts.append(row.encounter_counts)\n",
    "\n",
    "    avg_hf_encounter_count = np.mean(hf_encounter_counts)\n",
    "    max_hf_encounter_count = np.max(hf_encounter_counts)\n",
    "    min_hf_encounter_count = np.min(hf_encounter_counts)\n",
    "    avg_norm_encounter_count = np.mean(norm_encounter_counts)\n",
    "    max_norm_encounter_count = np.max(norm_encounter_counts)\n",
    "    min_norm_encounter_count = np.min(norm_encounter_counts)     \n",
    "\n",
    "    return avg_hf_encounter_count, max_hf_encounter_count, min_hf_encounter_count, \\\n",
    "           avg_norm_encounter_count, max_norm_encounter_count, min_norm_encounter_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to compute event count metrics: 1.9538638591766357s\n",
      "(188.9375, 2046, 28, 118.64423076923077, 1014, 6)\n",
      "Time to compute encounter count metrics: 2.034022331237793s\n",
      "(2.8060810810810812, 34, 2, 2.189423076923077, 11, 1)\n"
     ]
    }
   ],
   "source": [
    "events, hf = read_csv(TRAIN_DATA_PATH)\n",
    "\n",
    "#Compute the event count metrics\n",
    "start_time = time.time()\n",
    "event_count = event_count_metrics(events, hf)\n",
    "end_time = time.time()\n",
    "print((\"Time to compute event count metrics: \" + str(end_time - start_time) + \"s\"))\n",
    "print(event_count)\n",
    "\n",
    "#Compute the encounter count metrics\n",
    "start_time = time.time()\n",
    "encounter_count = encounter_count_metrics(events, hf)\n",
    "end_time = time.time()\n",
    "print((\"Time to compute encounter count metrics: \" + str(end_time - start_time) + \"s\"))\n",
    "print(encounter_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Feature construction\n",
    "\n",
    "It is a common practice to convert raw data into a standard data format before running machine learning models. Here we will implement the necessary python functions in this script and work with *events.csv*, *hf_events.csv* and *event_feature_map.csv* files provided in **TRAIN_DATA_PATH** folder.\n",
    "\n",
    "Some related concepts:\n",
    "\n",
    "<img src=\"img/window.jpg\" width=\"600\"/>\n",
    "\n",
    "- **Index vid**: Index vid is evaluated as follows:\n",
    "  - For heart failure patients: Index vid is the vid of the first visit with heart failure for that patient (i.e., vid field in *hf_events.csv*). \n",
    "  - For normal patients: Index vid is the vid of the last visit for that patient (i.e., vid field in *events.csv*). \n",
    "- **Observation Window**: The time interval you will use to identify relevant events. Only events present in this window should be included while constructing feature vectors.\n",
    "- **Prediction Window**: A fixed time interval that is to be used to make the prediction.\n",
    "\n",
    "In the example above, the index vid is 3. Visits with vid 0, 1, 2 are within the observation window. The prediction window is between visit 2 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Compute the index vid\n",
    "\n",
    "We will use the above definitions to compute the index vid for all patients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "\n",
    "def read_csv(filepath=TRAIN_DATA_PATH):\n",
    "    \n",
    "    '''\n",
    "    Read the events.csv, hf_events.csv and event_feature_map.csv files.\n",
    "    '''\n",
    "\n",
    "    events = pd.read_csv(filepath + 'events.csv')\n",
    "    hf = pd.read_csv(filepath + 'hf_events.csv')\n",
    "    feature_map = pd.read_csv(filepath + 'event_feature_map.csv')\n",
    "\n",
    "    return events, hf, feature_map\n",
    "\n",
    "\n",
    "def calculate_index_vid(events, hf):\n",
    "    \n",
    "    '''\n",
    "    Steps:\n",
    "        1. Create list of normal patients (hf_events.csv only contains information about heart failure patients).\n",
    "        2. Split events into two groups based on whether the patient has heart failure or not.\n",
    "        3. Calculate index vid for each patient.\n",
    "    '''\n",
    "\n",
    "    indx_vid = events[['pid', 'vid']].drop_duplicates(subset=['pid'], keep = 'last', ignore_index=True)\n",
    "    hf_pid_df = hf.set_index('pid')\n",
    "    hf_pids = list(hf.pid)\n",
    "    for index, row in indx_vid.iterrows():\n",
    "        if row.pid in hf_pids:\n",
    "            indx_vid.vid[index] = hf_pid_df.vid[row.pid]\n",
    "    indx_vid.rename(columns={\"vid\": \"indx_vid\"}, inplace=True)\n",
    "    \n",
    "    return indx_vid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Filter events\n",
    "\n",
    "Remove the events that occur outside the observation window. That is, all events in visits before index vid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_events(events, indx_vid):\n",
    "    '''\n",
    "    Steps:\n",
    "        1. Join indx_vid with events on pid.\n",
    "        2. Filter events occuring in the observation window [:, index vid).\n",
    "    '''\n",
    "    events_indx_vid = pd.merge(events, indx_vid, how=\"left\", on=[\"pid\"])\n",
    "    filtered_events = events_indx_vid[events_indx_vid.vid < events_indx_vid.indx_vid][['pid', 'event_id', 'value']].reset_index(drop=True)\n",
    "    return filtered_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Aggregate events\n",
    "\n",
    "To create features suitable for machine learning, we will need to aggregate the events for each patient as follows:\n",
    "\n",
    "- **count** occurences for each event.\n",
    "\n",
    "Each event type will become a feature and we will directly use event_id as feature name. For example, given below raw event sequence for a patient,\n",
    "\n",
    "```\n",
    "33,DIAG_244,0,1\n",
    "33,LAB_50971,0,1\n",
    "33,LAB_50931,0,1\n",
    "33,LAB_50931,0,1\n",
    "33,DIAG_244,1,1\n",
    "33,DIAG_427,1,1\n",
    "33,DRUG_0,1,1\n",
    "33,DRUG_3,1,1\n",
    "33,DRUG_3,1,1\n",
    "```\n",
    "\n",
    "We can get feature value pairs *(event_id, value)* for this patient with ID *33* as\n",
    "```\n",
    "(DIAG_244, 2.0)\n",
    "(LAB_50971, 1.0)\n",
    "(LAB_50931, 2.0)\n",
    "(DIAG_427, 1.0)\n",
    "(DRUG_0, 1.0)\n",
    "(DRUG_3, 2.0)\n",
    "```\n",
    "\n",
    "Next, replace each *event_id* with the *feature_id* provided in *event_feature_map.csv*.\n",
    "\n",
    "```\n",
    "(146, 2.0)\n",
    "(1434, 1.0)\n",
    "(1429, 2.0)\n",
    "(304, 1.0)\n",
    "(898, 1.0)\n",
    "(1119, 2.0)\n",
    "```\n",
    "\n",
    "Lastly, it is important to normalize different features into the same scale. We will use the [min-max normalization](http://stats.stackexchange.com/questions/70801/how-to-normalize-data-to-0-1-range) approach. (Note: we define $min(x)$ is always 0, i.e. the scale equation become $x$/$max(x)$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_events(filtered_events_df, hf_df, feature_map_df):\n",
    "    \n",
    "    '''\n",
    "    Steps:\n",
    "        1. Replace event_id's with index available in event_feature_map.csv.\n",
    "        2. Aggregate events using count to calculate feature value.\n",
    "        3. Normalize the values obtained above using min-max normalization(the min value will be 0 in all scenarios).\n",
    "    '''\n",
    "    filtered_events_df = pd.merge(filtered_events_df, feature_map, how=\"left\", on=[\"event_id\"])\n",
    "    filtered_events_df = filtered_events_df.rename(columns={'idx':'feature_id'})\n",
    "    aggregated_events = filtered_events_df.groupby(['pid', 'feature_id']).size().reset_index().rename(columns={0:'feature_value'})\n",
    "    feature_max = aggregated_events.groupby(['feature_id'])['feature_value'].max().reset_index().rename(columns={'feature_value':'max_value'})\n",
    "    aggregated_events = pd.merge(aggregated_events, feature_max, how = 'left', on = 'feature_id')\n",
    "    aggregated_events.feature_value = aggregated_events.feature_value/aggregated_events.max_value\n",
    "    aggregated_events.drop(columns = ['max_value'], inplace = True)\n",
    "    \n",
    "    return aggregated_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Save in  SVMLight format\n",
    "\n",
    "If the dimensionality of a feature vector is large but the feature vector is sparse (i.e. it has only a few nonzero elements), sparse representation should be employed. Here we will use the provided data for each patient to construct a feature vector and represent the feature vector in SVMLight format.\n",
    "\n",
    "```\n",
    "<line> .=. <target> <feature>:<value> <feature>:<value>\n",
    "<target> .=. 1 | 0\n",
    "<feature> .=. <integer>\n",
    "<value> .=. <float>\n",
    "```\n",
    "\n",
    "The target value and each of the feature/value pairs are separated by a space character. Feature/value pairs MUST be ordered by increasing feature number. **(using `save_svmlight()`.)** Features with value zero can be skipped. For example, the feature vector in SVMLight format will look like: \n",
    "\n",
    "```\n",
    "1 2:0.5 3:0.12 10:0.9 2000:0.3\n",
    "0 4:1.0 78:0.6 1009:0.2\n",
    "1 33:0.1 34:0.98 1000:0.8 3300:0.2\n",
    "1 34:0.1 389:0.32\n",
    "```\n",
    "\n",
    "where, 1 or 0 will indicate whether the patient has heart failure or not (i.e. the label) and it will be followed by a series of feature-value pairs **sorted** by the feature index (idx) value.\n",
    "\n",
    "The *utils.py* script will be useful here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load lib/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import collections\n",
    "\n",
    "def create_features(events_in, hf_in, feature_map_in):\n",
    "\n",
    "    indx_vid = calculate_index_vid(events_in, hf_in)\n",
    "\n",
    "    #Filter events in the observation window\n",
    "    filtered_events = filter_events(events_in, indx_vid)\n",
    "\n",
    "    #Aggregate the event values for each patient \n",
    "    aggregated_events = aggregate_events(filtered_events, hf_in, feature_map_in)\n",
    "\n",
    "    pid_is_hf = list(hf_in.pid)\n",
    "    pid_all = list(aggregated_events.pid.unique())\n",
    "    \n",
    "    patient_features, hf = {}, {}\n",
    "    for pid in pid_all:\n",
    "        patient_features[pid] = aggregated_events[aggregated_events.pid==pid].drop(columns=['pid']).to_records(index=False).tolist()\n",
    "    for pid in pid_is_hf:\n",
    "        hf[pid] = 1\n",
    "\n",
    "    return patient_features, hf\n",
    "\n",
    "def save_svmlight(patient_features, hf, op_file):\n",
    "\n",
    "    deliverable = open(op_file, 'wb')\n",
    "    hf_pids = hf.keys()\n",
    "    pids = sorted(patient_features.keys())\n",
    "    for pid in pids:\n",
    "        label = 1 if pid in hf_pids else 0\n",
    "        features = sorted(patient_features[pid])\n",
    "        feature_value = utils.bag_to_svmlight(features)\n",
    "        # save the files\n",
    "        deliverable.write(bytes(f\"{label} {feature_value} \\n\", 'utf-8'))\n",
    "    deliverable.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put together the whole pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    events_in, hf_in, feature_map_in = read_csv(TRAIN_DATA_PATH)\n",
    "    patient_features, hf = create_features(events_in, hf_in, feature_map_in)\n",
    "    save_svmlight(patient_features, hf, 'features_svmlight.train')\n",
    "    \n",
    "    events_in, hf_in, feature_map_in = read_csv(VAL_DATA_PATH)\n",
    "    patient_features, hf = create_features(events_in, hf_in, feature_map_in)\n",
    "    save_svmlight(patient_features, hf, 'features_svmlight.val')\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Predictive Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Model Creation\n",
    "\n",
    "Now we have constructed feature vectors for patients to be used as training data in various predictive models (classifiers). We can use this training data (*features_svmlight.train*) in 3 predictive models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step - a. Implement Logistic Regression, SVM and Decision Tree. Skeleton code is provided in the following code cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________\n",
      "Classifier: Logistic Regression\n",
      "Accuracy: 0.856338028169014\n",
      "Precision: 0.8357933579335793\n",
      "Recall: 0.937888198757764\n",
      "F1-score: 0.8839024390243903\n",
      "______________________________________________\n",
      "\n",
      "______________________________________________\n",
      "Classifier: SVM\n",
      "Accuracy: 0.9070422535211268\n",
      "Precision: 0.896484375\n",
      "Recall: 0.9503105590062112\n",
      "F1-score: 0.9226130653266331\n",
      "______________________________________________\n",
      "\n",
      "______________________________________________\n",
      "Classifier: Decision Tree\n",
      "Accuracy: 0.703420523138833\n",
      "Precision: 0.6657355679702048\n",
      "Recall: 0.9868875086266391\n",
      "F1-score: 0.7951070336391437\n",
      "______________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import *\n",
    "\n",
    "import utils\n",
    "\n",
    "RANDOM_STATE = 545510477\n",
    "\n",
    "\n",
    "def logistic_regression_pred(X_train, Y_train):\n",
    "    model = LogisticRegression(random_state=RANDOM_STATE).fit(X_train, Y_train)\n",
    "    return model.predict(X_train)\n",
    "\n",
    "def svm_pred(X_train, Y_train):\n",
    "    model = LinearSVC(random_state=RANDOM_STATE).fit(X_train, Y_train)\n",
    "    return model.predict(X_train)\n",
    "\n",
    "def decisionTree_pred(X_train, Y_train):\n",
    "    model = DecisionTreeClassifier(random_state=RANDOM_STATE, max_depth=5).fit(X_train, Y_train)\n",
    "    return model.predict(X_train)\n",
    "\n",
    "\n",
    "def classification_metrics(Y_pred, Y_true):\n",
    "    tn, fp, fn, tp = confusion_matrix(Y_true, Y_pred).ravel()\n",
    "    acc = (tp + tn)/(tn + fp + fn + tp)\n",
    "    precision = tp/(tp + fp)\n",
    "    recall = tp/(tp + fn)\n",
    "    f1score = 2*precision*recall/(precision+recall)\n",
    "    \n",
    "    return acc, precision, recall, f1score\n",
    "\n",
    "\n",
    "def display_metrics(classifierName, Y_pred, Y_true):\n",
    "    print(\"______________________________________________\")\n",
    "    print((\"Classifier: \"+classifierName))\n",
    "    acc, precision, recall, f1score = classification_metrics(Y_pred,Y_true)\n",
    "    print((\"Accuracy: \"+str(acc)))\n",
    "    print((\"Precision: \"+str(precision)))\n",
    "    print((\"Recall: \"+str(recall)))\n",
    "    print((\"F1-score: \"+str(f1score)))\n",
    "    print(\"______________________________________________\")\n",
    "    print(\"\")\n",
    "\n",
    "    \n",
    "def main():\n",
    "    X_train, Y_train = utils.get_data_from_svmlight(\"features_svmlight.train\")\n",
    "\n",
    "    display_metrics(\"Logistic Regression\", logistic_regression_pred(X_train, Y_train), Y_train)\n",
    "    display_metrics(\"SVM\",svm_pred(X_train, Y_train),Y_train)\n",
    "    display_metrics(\"Decision Tree\", decisionTree_pred(X_train, Y_train), Y_train)\n",
    "\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step - b. Evaluate your predictive models on a separate test dataset in *features_svmlight.val* (binary labels are provided in that svmlight file as the first field). Skeleton code is provided in the following code cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T04:30:19.404126Z",
     "start_time": "2022-01-17T04:30:19.123750Z"
    },
    "deletable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________\n",
      "Classifier: Logistic Regression\n",
      "Accuracy: 0.6937086092715232\n",
      "Precision: 0.7345360824742269\n",
      "Recall: 0.776566757493188\n",
      "F1-score: 0.7549668874172186\n",
      "______________________________________________\n",
      "\n",
      "______________________________________________\n",
      "Classifier: SVM\n",
      "Accuracy: 0.640728476821192\n",
      "Precision: 0.7038043478260869\n",
      "Recall: 0.7057220708446866\n",
      "F1-score: 0.7047619047619047\n",
      "______________________________________________\n",
      "\n",
      "______________________________________________\n",
      "Classifier: Decision Tree\n",
      "Accuracy: 0.6821192052980133\n",
      "Precision: 0.6611418047882136\n",
      "Recall: 0.9782016348773842\n",
      "F1-score: 0.789010989010989\n",
      "______________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    X_train, Y_train = utils.get_data_from_svmlight(\"features_svmlight.train\")\n",
    "    X_test, Y_test = utils.get_data_from_svmlight(os.path.join(\"features_svmlight.val\"))\n",
    "\n",
    "    display_metrics(\"Logistic Regression\", logistic_regression_pred(X_train, Y_train, X_test), Y_test)\n",
    "    display_metrics(\"SVM\", svm_pred(X_train, Y_train, X_test), Y_test)\n",
    "    display_metrics(\"Decision Tree\", decisionTree_pred(X_train, Y_train, X_test), Y_test)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Model Validation\n",
    "\n",
    "In order to fully utilize the available data and obtain more reliable results, we use cross-validation to evaluate and improve their predictive models. \n",
    "\n",
    "- K-fold: Divide all the data into $k$ groups of samples. Each time $\\frac{1}{k}$ samples will be used as test data and the remaining samples as training data.\n",
    "- Randomized K-fold: Iteratively random shuffle the whole dataset and use top specific percentage of data as training and the rest as test. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement the two cross-validation strategies.**\n",
    "- **K-fold:** Use the number of iterations k=5; \n",
    "- **Randomized K-fold**: Use a test data percentage of 20\\% and k=5 for the number of iterations for Randomized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: SVD\n",
      "Average F1 Score in KFold CV: 0.7258461959533061\n",
      "Average F1 Score in Randomised CV: 0.7195678940019832\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, ShuffleSplit\n",
    "from numpy import mean\n",
    "\n",
    "import utils\n",
    "\n",
    "RANDOM_STATE = 545510477\n",
    "\n",
    "def get_f1_kfold(X, Y, k=5):\n",
    "    \n",
    "    \"\"\"\n",
    "    First get the train indices and test indices for each iteration.\n",
    "    Then train the classifier accordingly.\n",
    "    Report the mean f1 score of all the folds.\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(Y)\n",
    "    size = n//k\n",
    "    f1scores = []\n",
    "    for i in range(k):\n",
    "        test_idx = [idx for idx in range(i*size,min((i+1)*size, n))]\n",
    "        train_idx = [idx for idx in range(i*size)]\n",
    "        train_idx += [idx for idx in range(min((i+1)*size, n),n)]\n",
    "        X_test = X[test_idx,]\n",
    "        Y_test = Y[test_idx]\n",
    "        X_train = X[train_idx,]\n",
    "        Y_train = Y[train_idx]                \n",
    "        model = LinearSVC(random_state=RANDOM_STATE).fit(X_train, Y_train)\n",
    "        Y_pred = model.predict(X_test)\n",
    "        acc, precision, recall, f1score = classification_metrics(Y_pred,Y_test)\n",
    "        f1scores.append(f1score)\n",
    "    return np.mean(f1scores)\n",
    "\n",
    "def get_f1_randomisedCV(X, Y, iterNo=5, test_percent=0.20):\n",
    "\n",
    "    \"\"\"\n",
    "    First get the train indices and test indices for each iteration.\n",
    "    Then train the classifier accordingly.\n",
    "    Report the mean f1 score of all the iterations.\n",
    "    \"\"\"\n",
    "    f1scores = []\n",
    "    rs = ShuffleSplit(n_splits=iterNo, test_size=test_percent, random_state=RANDOM_STATE)\n",
    "    rs.get_n_splits(X)\n",
    "    for train_index, test_index in rs.split(X):            \n",
    "        X_test = X[test_index,]\n",
    "        Y_test = Y[test_index]\n",
    "        X_train = X[train_index,]\n",
    "        Y_train = Y[train_index]                \n",
    "        model = LinearSVC(random_state=RANDOM_STATE).fit(X_train, Y_train)\n",
    "        Y_pred = model.predict(X_test)\n",
    "        acc, precision, recall, f1score = classification_metrics(Y_pred,Y_test)\n",
    "        f1scores.append(f1score)\n",
    "    return np.mean(f1scores)    \n",
    "\n",
    "    \n",
    "def main():\n",
    "    X,Y = utils.get_data_from_svmlight(\"features_svmlight.train\")\n",
    "    print(\"Classifier: SVD\")\n",
    "    f1_k = get_f1_kfold(X,Y)\n",
    "    print((\"Average F1 Score in KFold CV: \"+str(f1_k)))\n",
    "    f1_r = get_f1_randomisedCV(X,Y)\n",
    "    print((\"Average F1 Score in Randomised CV: \"+str(f1_r)))\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "832px",
    "left": "419px",
    "top": "110px",
    "width": "311.390625px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
